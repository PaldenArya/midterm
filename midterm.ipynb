{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Midterm Project \u2014 Decision Trees on Breast Cancer Diagnostic (WDBC)\n",
        "\n",
        "**Author:** Palden Arya  \n",
        "**Task:** Supervised classification with numeric features and a categorical target.  \n",
        "**Dataset:** Breast Cancer Wisconsin (Diagnostic) \u2014 loaded from `sklearn.datasets` (UCI source).\n",
        "\n",
        "This notebook reproduces the full workflow required by the assignment:\n",
        "- Data description\n",
        "- Cleaning & preprocessing\n",
        "- Preliminary analysis & EDA (plots)\n",
        "- Three decision tree models (low / high / medium via grid search)\n",
        "- Over/underfitting comparison\n",
        "- Ten-percent-worse model\n",
        "- Evaluation metrics & confidence intervals\n",
        "- Brief interpretation\n",
        "\n",
        "> Note: Figures and outputs are saved into `./figures` and `./outputs` for easy download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# If running in Google Colab, uncomment the next line to install dependencies:\n",
        "# !pip install scikit-learn matplotlib pandas reportlab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "import os, math, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "os.makedirs(\"figures\", exist_ok=True)\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "plt.rcParams.update({\"figure.figsize\": (6,4)})"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "data_bunch = load_breast_cancer(as_frame=True)\n",
        "X_full = data_bunch.data.copy()\n",
        "y_full = data_bunch.target.copy()\n",
        "feature_names = list(X_full.columns)\n",
        "target_mapping = {0: \"malignant\", 1: \"benign\"}\n",
        "\n",
        "df = X_full.copy()\n",
        "df[\"target\"] = y_full.map(target_mapping)\n",
        "df[\"_target_int\"] = y_full\n",
        "\n",
        "print(\"Rows/Cols:\", df.shape)\n",
        "print(\"Classes:\", df[\"target\"].value_counts().to_dict())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Type enforcement + missing checks; impute with median if needed\n",
        "for col in feature_names:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    if df[col].isna().any():\n",
        "        df[col] = df[col].fillna(df[col].median())\n",
        "\n",
        "missing_counts = df.isna().sum()\n",
        "print(\"Missing values per column (should be 0):\")\n",
        "print(missing_counts[missing_counts>0] if (missing_counts>0).any() else \"None\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "describe_df = df[feature_names].describe().T\n",
        "display(describe_df.head(12))\n",
        "\n",
        "class_counts = df[\"target\"].value_counts().rename_axis(\"class\").reset_index(name=\"count\")\n",
        "class_counts[\"proportion\"] = class_counts[\"count\"] / class_counts[\"count\"].sum()\n",
        "display(class_counts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def save_fig(path):\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "# Class distribution\n",
        "plt.figure()\n",
        "plt.pie(class_counts[\"count\"], labels=class_counts[\"class\"], autopct=\"%1.1f%%\")\n",
        "plt.title(\"Class Distribution\")\n",
        "save_fig(\"figures/pie_class_distribution.png\")\n",
        "\n",
        "# Density of a key feature\n",
        "plt.figure()\n",
        "plt.hist(df[\"mean radius\"], bins=30, density=True)\n",
        "plt.xlabel(\"mean radius\"); plt.ylabel(\"density\"); plt.title(\"Density: mean radius\")\n",
        "save_fig(\"figures/density_mean_radius.png\")\n",
        "\n",
        "# Scatter matrix (subset)\n",
        "subset_cols = [\"mean radius\", \"mean texture\", \"mean perimeter\", \"mean area\"]\n",
        "pd.plotting.scatter_matrix(df[subset_cols], figsize=(8,8))\n",
        "plt.suptitle(\"Scatter Matrix (subset)\", y=1.02)\n",
        "save_fig(\"figures/scatter_matrix_subset.png\")\n",
        "\n",
        "# Boxplots vs target\n",
        "for col in subset_cols:\n",
        "    plt.figure()\n",
        "    data_mal = df[df[\"target\"]==\"malignant\"][col].values\n",
        "    data_ben = df[df[\"target\"]==\"benign\"][col].values\n",
        "    plt.boxplot([data_mal, data_ben], labels=[\"malignant\",\"benign\"])\n",
        "    plt.title(f\"Boxplot: {col} by target\")\n",
        "    plt.ylabel(col)\n",
        "    save_fig(f\"figures/boxplot_{col.replace(' ','_')}_by_target.png\")\n",
        "\n",
        "print(\"Saved figures to ./figures\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "X = df[feature_names].values\n",
        "y = df[\"_target_int\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "tree_high = DecisionTreeClassifier(random_state=42)\n",
        "tree_high.fit(X_train, y_train)\n",
        "\n",
        "tree_low = DecisionTreeClassifier(max_depth=1, min_samples_leaf=10, random_state=42)\n",
        "tree_low.fit(X_train, y_train)\n",
        "\n",
        "def eval_model(name, model):\n",
        "    ytr = model.predict(X_train); yte = model.predict(X_test)\n",
        "    return {\n",
        "        \"model\": name,\n",
        "        \"train_acc\": accuracy_score(y_train, ytr),\n",
        "        \"test_acc\": accuracy_score(y_test, yte),\n",
        "        \"train_bal_acc\": balanced_accuracy_score(y_train, ytr),\n",
        "        \"test_bal_acc\": balanced_accuracy_score(y_test, yte),\n",
        "    }\n",
        "\n",
        "scores = [eval_model(\"high_unrestricted\", tree_high), eval_model(\"low_complexity\", tree_low)]\n",
        "pd.DataFrame(scores)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, None],\n",
        "    \"min_samples_split\": [2, 4, 8],\n",
        "    \"min_samples_leaf\": [1, 2, 4, 8],\n",
        "    \"criterion\": [\"gini\", \"entropy\"]\n",
        "}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=cv,\n",
        "    n_jobs=1,\n",
        "    refit=True,\n",
        "    return_train_score=False\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "best_tree = grid.best_estimator_\n",
        "best_params = grid.best_params_\n",
        "best_cv_score = grid.best_score_\n",
        "\n",
        "scores.append(eval_model(\"medium_grid_best\", best_tree))\n",
        "scores_df = pd.DataFrame(scores)\n",
        "display(scores_df)\n",
        "print(\"Best params:\", best_params)\n",
        "print(\"CV mean accuracy:\", best_cv_score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "A = scores_df.loc[scores_df['model']=='high_unrestricted','test_acc'].iloc[0]\n",
        "threshold = A - 0.10\n",
        "\n",
        "def node_count(dt):\n",
        "    return dt.tree_.node_count\n",
        "\n",
        "candidates = []\n",
        "for d in [1, 2, 3, 4, 5, None]:\n",
        "    for leaf in [1, 2, 4, 8, 16, 32]:\n",
        "        m = DecisionTreeClassifier(max_depth=d, min_samples_leaf=leaf, random_state=42)\n",
        "        m.fit(X_train, y_train)\n",
        "        acc = accuracy_score(y_test, m.predict(X_test))\n",
        "        if acc >= threshold:\n",
        "            candidates.append({\"d\": d if d is not None else \"None\", \"leaf\": leaf, \"nodes\": node_count(m), \"test_acc\": acc, \"model\": m})\n",
        "\n",
        "if candidates:\n",
        "    candidates.sort(key=lambda c: (c[\"nodes\"], -c[\"test_acc\"]))\n",
        "    chosen = candidates[0]\n",
        "    ten_model = chosen[\"model\"]\n",
        "    ten_meta = {k:v for k,v in chosen.items() if k!=\"model\"}\n",
        "else:\n",
        "    ten_model = DecisionTreeClassifier(max_depth=2, random_state=42).fit(X_train, y_train)\n",
        "    ten_meta = {\"d\": 2, \"leaf\": 1, \"nodes\": node_count(ten_model), \"test_acc\": accuracy_score(y_test, ten_model.predict(X_test)), \"note\": \"Fallback\"}\n",
        "\n",
        "def metrics(name, model):\n",
        "    ypred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, ypred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    return {\n",
        "        \"model\": name,\n",
        "        \"accuracy\": accuracy_score(y_test, ypred),\n",
        "        \"balanced_accuracy\": balanced_accuracy_score(y_test, ypred),\n",
        "        \"precision\": precision_score(y_test, ypred, zero_division=0),\n",
        "        \"recall\": recall_score(y_test, ypred, zero_division=0),\n",
        "        \"false_negatives\": int(fn),\n",
        "        \"confusion_matrix\": cm.tolist()\n",
        "    }\n",
        "\n",
        "met_low  = metrics(\"low_complexity\", tree_low)\n",
        "met_med  = metrics(\"medium_grid_best\", best_tree)\n",
        "met_high = metrics(\"high_unrestricted\", tree_high)\n",
        "met_ten  = metrics(\"ten_percent_worse\", ten_model)\n",
        "\n",
        "summary = {\n",
        "    \"scores\": scores_df.to_dict(orient=\"records\"),\n",
        "    \"grid_best_params\": best_params,\n",
        "    \"grid_best_cv_mean_accuracy\": float(best_cv_score),\n",
        "    \"metrics\": [met_low, met_med, met_high, met_ten],\n",
        "    \"ten_percent_worse_meta\": ten_meta\n",
        "}\n",
        "with open(\"outputs/results_summary.json\",\"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "display(pd.DataFrame(summary[\"metrics\"]))\n",
        "print(\"10%-worse meta:\", ten_meta)\n",
        "print(\"Saved outputs/results_summary.json\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def summarize_depth1(model, feat_names, labels={0:'malignant',1:'benign'}):\n",
        "    tr = model.tree_\n",
        "    if tr.max_depth != 1:\n",
        "        return {\"note\": \"not depth=1\"}\n",
        "    fidx = tr.feature[0]\n",
        "    thr = tr.threshold[0]\n",
        "    left = np.argmax(tr.value[tr.children_left[0]][0])\n",
        "    right = np.argmax(tr.value[tr.children_right[0]][0])\n",
        "    return {\n",
        "        \"comparisons_made\": 1,\n",
        "        \"if_rule\": f\"If {feat_names[fidx]} <= {thr:.4f} then class={labels[left]}\",\n",
        "        \"else_rule\": f\"Else class={labels[right]}\"\n",
        "    }\n",
        "\n",
        "low_rules = summarize_depth1(DecisionTreeClassifier(max_depth=1, min_samples_leaf=10, random_state=42).fit(X_train, y_train), feature_names)\n",
        "low_rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def bootstrap_ci_acc(model, Xte, yte, B=500, alpha=0.05, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(yte)\n",
        "    preds = model.predict(Xte)\n",
        "    accs = []\n",
        "    for _ in range(B):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        accs.append(accuracy_score(yte[idx], preds[idx]))\n",
        "    return float(np.quantile(accs, alpha/2)), float(np.quantile(accs, 1-alpha/2))\n",
        "\n",
        "ci_med = bootstrap_ci_acc(best_tree, X_test, y_test)\n",
        "ci_ten = bootstrap_ci_acc(ten_model, X_test, y_test)\n",
        "\n",
        "print(\"95% CI (accuracy) \u2014 medium:\", ci_med)\n",
        "print(\"95% CI (accuracy) \u2014 10%-worse:\", ci_ten)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Optional: Create a compact PDF report (uncomment to run)\n",
        "# from reportlab.lib.pagesizes import letter\n",
        "# from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle\n",
        "# from reportlab.lib.styles import getSampleStyleSheet\n",
        "# from reportlab.lib import colors\n",
        "# import json, os\n",
        "#\n",
        "# with open(\"outputs/results_summary.json\") as f:\n",
        "#     res = json.load(f)\n",
        "# pdf_path = \"outputs/Midterm_Report_PaldenArya.pdf\"\n",
        "# doc = SimpleDocTemplate(pdf_path, pagesize=letter)\n",
        "# styles = getSampleStyleSheet()\n",
        "# elements = [Paragraph(\"Data Science Midterm Project \u2014 Palden Arya\", styles[\"Title\"]), Spacer(1,12)]\n",
        "# # (Add tables/figures similar to the script we used earlier)\n",
        "# doc.build(elements)\n",
        "# print(\"Saved PDF:\", pdf_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}